---
title: "Modelling spatial variability of Municipal Council election turnout in the Netherlands"
author: "Max van den Elsen (2590611), Sander Engelberts (1422138), and Thom Venema (1157485)"
date: 'April 2022'
output: html_document
---
Term project for the MSc Applied Data Science course Spatial statistics and machine learning at Utrecht University. 

The code in this notebook is inspired by provided code in the practicals of this course, created by its teachers. 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading required libraries
This code block loads the required libraries and will ask to install them when these aren't yet.
```{r, , include=FALSE}
# include=FALSE makes sure the output of loading/installing isn't printed

# Install packages if required
if (!require("tidymodels")) install.packages("tidymodels") # For creating modelling workflows
if (!require("easypackages")) install.packages("easypackages") # For easy loading of packages
if (!require("sf")) install.packages("sf") #main GIS package
if (!require("sp")) install.packages("sp") #needed for some GIS operation, will not be in use from 2023
if (!require("spdep")) install.packages("spdep") # Neighborhood analysis in R
if (!require("spatialreg")) install.packages("spatialreg") # Spatial modelling such as lag, error
if (!require("spatialsample")) install.packages("spatialsample") # Spatial cross validation
if (!require("ranger")) install.packages("ranger") # Grid tuning
if (!require("spgwr")) install.packages("spgwr") # GWR modelling
if (!require("RColorBrewer")) install.packages("RColorBrewer") # Getting interesting color
if (!require("tmap")) install.packages("tmap") # Mapping package
if (!require("mapview")) install.packages("mapview") # Mapping package
if (!require("car")) install.packages("car") # Some base regression functions
if (!require("cowplot")) install.packages("cowplot") # Some base regression functions
if (!require("leafsync")) install.packages("leafsync") # Using with mapview
if (!require("leaflet.extras2")) install.packages("leaflet.extras2") #using with mapview
if (!require("modelStudio")) install.packages("modelStudio") # Creating interactive model explaining dashboard for random forest
if (!require("DALEX")) install.packages("DALEX") # Exploring feature importance based on permutation for random forest
if (!require("DALEXtra")) install.packages("DALEXtra") # Exploring feature importance based on permutation for random forest
if (!require("vip")) install.packages("vip") # Exploring feature importance based on impurity for random forest
if (!require("performance")) install.packages("performance") # For checking assumptions linear regression
if (!require("see")) install.packages("see") # For checking assumptions linear regression

# Load packages
easypackages::packages ("tidymodels", "sf", "sp", "spdep", "spatialreg", "spatialsample", "spgwr", "ranger", "tmap", "mapview", "car", "RColorBrewer", "tidyverse", "cowplot", "leafsync", "leaflet.extras2", "mapview", "modelStudio", "DALEX", "DALEXtra", "vip", "performance", "see") 

# Clean R environment
rm(list=ls())
```

## Load and inspect processed data
The Municipal Council election turnout in the Netherlands of 2022 has been processed into a dataset with the file `opkomst_per_stembureau.R`. This coupled the turnout at polling stations to their location and cleaned the data. Next, this data was aggregated to neighbourhoods in the Netherlands using Thiessen polygons around the polling stations and a weighted average over these polygons a neighbourhood area overlaps. For these neighbourhoods also the independent variables were retrieved from CBS to create the final dataset that will be used for our models. Here now first load and inspect that data.

```{r}
# Loading ShapeFile with processed data
neighbourhood_data <- read_sf(file.path("wijk_statistics", "CBS_turnout_Wijk.shp"))

# Check the variables
names(neighbourhood_data) # TODO: add an explanation of each variable underneath and preferably change the names to something more sensible and not such a short abbreviation
```

```{r}
# Change the parameter names to recognisable strings
newnames <- c('ID', 'wijkcode', 'gem_naam', 'bev_dichth_km2', 'aantal_inw', 'man', 'vrouw', 
             'p_0_15_jaar', 'p_15_25_jaar', 'p_25_45_jaar', 'p_45_65_jaar', 'p_65_jaar',
             'p_ongehuwd', 'p_gehuwd', 'p_gescheiden', 'p_verweduwd', 'p_ia_west', 'p_ia_n_west',
             'aantal_bedrijf', 'gem_woning_waarde', 'aant_ink_ont', 'gem_ink_p_o',
             'p_laag_ink', 'p_hoog_ink', 'aow_uitk', 'afst_levensm_5km', 'voortg_ond_3km', 'tot_uitkering', 'turnout_ratio', 'geometry') # Note that tot_uitkering doesn't include aow_uitk, just different ones that are e.g. because of not being able to work

names(neighbourhood_data) <- newnames
```

```{r}
# Plot the full dataset
plot(neighbourhood_data$geometry)
```

```{r}
# Drop variables that aren't relevant due to multicollinearity with another variable or due to many missing values (which can't be reliably imputed)


# Drop rows that contain one or multiple NA value(S) because otherwise the models can't be trained TODO: remove ones without turnout and decide which variables to drop
neighbourhood_data <- na.omit(neighbourhood_data)
```

```{r}
# Show data preview
neighbourhood_data 
```

```{r}
# Plot the neighbourhoods
plot(neighbourhood_data$geometry) 
```

```{r}
# Explore the data distributions
# Plot the turnout data
turnout_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="green3", aes(x = turnout_ratio)) +
  xlab("Election turnout fraction") +
  theme_classic()

# Plot the population density variable # TODO: use the actual predictors and add the other interesting ones as well
pop_density_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = bev_dichth_km2)) +
  xlab("Population per km^2") +
  theme_classic()

male_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = man)) +
  xlab("Registered male sex fraction") +
  theme_classic()

female_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = vrouw)) +
  xlab("Registered female sex fraction") +
  theme_classic()

p0_15_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_0_15_jaar)) +
  xlab("Percentage age 0-15 year") +
  theme_classic()

p15_25_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_15_25_jaar)) +
  xlab("Percentage age 15-25 year") +
  theme_classic()

p25_45_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_25_45_jaar)) +
  xlab("Percentage age 25-45 year") +
  theme_classic()

p45_65_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_45_65_jaar)) +
  xlab("Percentage age 45-65 year") +
  theme_classic()

p65_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_65_jaar)) +
  xlab("Percentage age >65 year") +
  theme_classic()

p_unmarried_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_ongehuwd)) +
  xlab("Percentage unmarried") +
  theme_classic()

p_married_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_gehuwd)) +
  xlab("Percentage married") +
  theme_classic()

p_divorced_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_gescheiden)) +
  xlab("Percentage divorced") +
  theme_classic()

p_widow_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_verweduwd)) +
  xlab("Percentage widow") +
  theme_classic()

p_western_imm_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_ia_west)) +
  xlab("Percentage western immigrant") +
  theme_classic()

p_not_western_imm_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_ia_n_west)) +
  xlab("Percentage not western immigrant") +
  theme_classic()

n_company_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = aantal_bedrijf)) +
  xlab("Number of companies") +
  theme_classic()

avg_house_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = gem_woning_waarde)) +
  xlab("Average house price (*1000)") +
  theme_classic()

n_working_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = aant_ink_ont)) +
  xlab("Number of income earners in household") +
  theme_classic()

avg_income_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = gem_ink_p_o)) +
  xlab("Average income of income earners (*1000)") +
  theme_classic()

p_low_income_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_laag_ink)) +
  xlab("Percentage of low income households") +
  theme_classic()

p_high_income_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = p_hoog_ink)) +
  xlab("Percentage of high income households") +
  theme_classic()

p_aow_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = aow_uitk)) +
  xlab("Percentage of people with AOW") +
  theme_classic()

d_supermarked_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = afst_levensm_5km)) +
  xlab("Distance to closest supermarket within 5km") +
  theme_classic() # TODO: is this and highschool one the number within 5km??

d_highschool_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = voortg_ond_3km)) +
  xlab("Distance to closest highschool within 3km") +
  theme_classic()

p_uitkering_plot <- ggplot(data = neighbourhood_data) +
  geom_density(alpha=0.8, colour="black", fill="lightblue", aes(x = tot_uitkering)) +
  xlab("Percentage of people with AO, WW, or algemene bijstands uitkering") +
  theme_classic()

# TODO: antl_nw is not interesting for us right as we used it to determine percentages?

# Show the density plots of interesting variables in a grid # TODO: may need to set how many displayed in a row and such
plot_grid(turnout_plot, pop_density_plot, male_plot, female_plot, p0_15_plot, p15_25_plot, p25_45_plot, p45_65_plot, p65_plot, p_unmarried_plot, p_married_plot, p_divorced_plot, p_widow_plot, p_western_imm_plot, p_not_western_imm_plot, n_company_plot, avg_house_plot, n_working_plot, avg_income_plot, p_low_income_plot, p_high_income_plot, p_aow_plot, d_supermarked_plot, d_highschool_plot, labels = "AUTO")
```

```{r}
# Map the dependent variable turnout
turnout_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "turnout_ratio", 
                                col.regions=brewer.pal(9, "YlOrRd"),
                                at = c(0,0.2,0.4,0.6,0.8,1,1.5,2)) # TODO: for other vars also check if I need to set/change some values for the ranges as outliers make the ranges too broad so all same colour in map

# Map interesting independent variables to compare with the spatial distribution of the independent variable # TODO add the other variables as well
pop_density_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "bev_dichth_km2", 
                                col.regions=brewer.pal(9, "YlOrRd"),
                                at = c(0,250,500,750,1000,2500,5000,10000))

male_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "man", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.3,0.4,0.5,0.6,0.7,1.0))

female_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "vrouw", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.3,0.4,0.5,0.6,0.7,1.0))

age_0_15_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_0_15_jaar", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

age_15_25_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_15_25_jaar", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

age_25_45_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_25_45_jaar", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

age_45_65_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_45_65_jaar", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

age_65_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_65_jaar", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

unmarried_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_ongehuwd", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.3,0.4,0.5,0.6,0.7,1.0))

married_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_gehuwd", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.3,0.4,0.5,0.6,0.7,1.0))

divorced_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_gescheiden", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

widow_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_verweduwd", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

western_imm_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_ia_west", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

not_western_imm_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_ia_n_west", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

company_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "aantal_bedrijf", 
                                col.regions=brewer.pal(9, "YlOrRd"),
                                at = c(0,250,500,750,1000,2500,5000,10000))

house_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "gem_woning_waarde", 
                                col.regions=brewer.pal(9, "YlOrRd"),
                                at = c(0,150,200,250,300,400,600,1000))

p_income_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "aant_ink_ont", 
                                col.regions=brewer.pal(9, "YlOrRd"),
                                at = c(0,0.25,0.5,0.75,1.0,1.25,1.5,2))

income_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "gem_ink_p_o", 
                                col.regions=brewer.pal(9, "YlOrRd"),
                                at = c(0,20,25,30,35,40,50,100))

low_income_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_laag_ink", 
                                col.regions=brewer.pal(9, "YlOrRd"),
                                at = c(0,0.2,0.3,0.4,0.5,0.6,0.7,1.0))

high_income_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "p_hoog_ink", 
                                col.regions=brewer.pal(9, "YlOrRd"),
                                at = c(0,0.2,0.3,0.4,0.5,0.6,0.7,1.0))

aow_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "aow_uitk", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))

supermarket_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "afst_levensm_5km", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,20,40,60,80,100,250))

highschool_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "voortg_ond_3km", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,20,40,60,80,100,250))

uitkering_map <- mapview::mapview(neighbourhood_data, 
                                zcol = "tot_uitkering", 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(0,0.1,0.2,0.3,0.4,0.5,1.0))
```

```{r}
# Display two maps with a slider to compare the spatial distribution of the variables in all the neighborhoods # TODO: do this for combinations of all other interesting independent variables against the turnout (and maybe some relations between some of the predictor variables)
#turnout_map | pop_density_map # TODO: does work when just showing one map, but maybe due to the missing values not being in this newest data
turnout_map

pop_density_map

male_map

female_map

age_0_15_map

age_15_25_map

age_25_45_map

age_45_65_map

age_65_map

unmarried_map

married_map

divorced_map
```

```{r}
widow_map

western_imm_map

not_western_imm_map

company_map

house_map

p_income_map

income_map

low_income_map

high_income_map

aow_map

supermarket_map

highschool_map
```

## Pre-process data
Pre-process the data by creating a training and test dataset, normalizing the variables, and creating a model recipe with the model equation. Additionally,  cross validation splits will be created with one taking into account spatial coordinates and the other one not, to check for difference in tuned parameters and model performance.

```{r}
# Set seed for reproducible results
set.seed(123)

# TODO: potentially drop geometry column if exists and needed (was just for easy handling in the model they said)

# Determine coordinates of neighbourhood centroids to split on with spatial cross validation
centroid_coords <- st_coordinates(st_centroid(neighbourhood_data$geometry))
# Need to get integers instead of doubles, and add to dataframe
neighbourhood_data$X_centroid <- centroid_coords[,c("X")]
neighbourhood_data$Y_centroid <- centroid_coords[,c("Y")] # TODO: is this in the end still used??

# Split the data into a training (75%) and test (25%) set using stratification on the dependent variable (turnout) to have a representative test sample
data_split <- rsample::initial_split(neighbourhood_data, strata = "turnout_ratio", prop = 0.75)
train_data <- rsample::training(data_split)
test_data  <- rsample::testing(data_split)

# Declare the set explicit
train.set <- train_data
test.set <- test_data
```

```{r}
# For the training dataset, create cross validation splits
# Create split without spatial cross-validation, but consider the dependent variable on which to stratify. Here the number of samples per fold is relatively equal, whereas with spatial cross-validation this is less the case
cv_splits <- rsample::vfold_cv(train.set, strata = "turnout_ratio", k = 10) #here K is the number of fold, k=10 is ten fold CV
print(cv_splits)
```

```{r}
# Create split with spatial cross-validation by clustering coordinate information
cv_spatial_folds <- spatial_clustering_cv(data.frame(train.set), coords = c("X_centroid", "Y_centroid"), v = 10)
print (cv_spatial_folds) 
```

Now the data is ready, the model recipe is defined.
```{r}
# Set the equation model recipe for predicting the dependent variable turnout
model_eq <- turnout_ratio ~ bev_dichth_km2 + man + vrouw + p_0_15_jaar + p_15_25_jaar + p_25_45_jaar + p_45_65_jaar + p_65_jaar + p_ongehuwd + p_gehuwd + p_gescheiden + p_verweduwd + p_ia_west + p_ia_n_west + aantal_bedrijf + gem_woning_waarde + aant_ink_ont + gem_ink_p_o + p_laag_ink + p_hoog_ink + aow_uitk + afst_levensm_5km + voortg_ond_3km + tot_uitkering
model_rec <- recipe(model_eq, data = train.set) %>%
  # Normalize the predictor variables using z-normalization
  step_zv(all_predictors()) %>%
  # Centralize some predictors # TODO: example did this only for some
  step_center(all_predictors()) %>%
  # Scale predictors with large values # TODO: example did this only for some
  step_scale(all_predictors())

# Inspect the final recipe by first preparing and then juicing it using glimpse, which shows the model structure for some samples
glimpse(model_rec %>% prep() %>% juice())
```

## Set up workflows for training models
Create the model workflows which will be used for training and predicting different models using the recipe, and set up grid search with a cross validation process for hyperparameter tuning. This then results in the final, optimal models to employ. Below first the model plans are created for linear regression, geographically weighted regression, and random forest models.

```{r}
#' Train Geographically Weighted Regression (GWR) model with an adaptive kernel.
#' This can't be done using the tidymodels package like the Linear regression and Random forest will use afterwards
#'
#' @param data Dataset with observations to fit
#' @param model_recipe tidymodels recipe describing the equation to determine relations for
#' @param weight_func gwr weight function to use for the adaptive kernel (default gwr.Gauss)
#' 
#' @return (gwr_model, k_NN) the trained GWR model and the number of nearest neighbourhoods to consider in the adaptive kernel
train_GWR <- function(data, model_recipe, weight_func=gwr.Gauss){
  # Before doing analysis for GWR the sf polygon object must be converted into a sp spatial object because the spgwr package usually works on sp objects
  data_sp <- as_Spatial(data)
  
  # Select optimal bandwidth for adaptive kernel (so k nearest neighbourhoods)
  bandwidth <- gwr.sel(prep(model_recipe), 
                data = data_sp,
                longlat = TRUE, # TODO: may be that we need to change CRS to WG84 from RD New
                adapt = TRUE, 
                gweight = weight_func) # TODO: change to bivariate likely to not let neighbourhoods further away count as much, and also make spatial lag model
  
  # Determine number nearest neighbourhood observations to include 
  k_NN <- bandwidth * nrow(data)
  print(paste("Adaptive kernel with number of nearest neighbourhoods: ", k_NN, " of total number of neighbourhoods: ", nrow(data)))
  
  # Train Geographically weighted regression model
  gwr_model <- gwr(prep(model_recipe), data = data_sp, longlat = TRUE, gweight = weight_func, hatmatrix=TRUE, se.fit=TRUE, bandwidth = bandwidth)
  
  # Show summary of Geographically weighted regression model
  gwr_model
  
  return (gwr_model)
}
```

```{r}
# TODO: fix that GWR works and the RMSE and such can still be plotted/ AIC determined (for the tidymodelsand perhaps also this one likely with glance function from broom library https://broom.tidymodels.org/articles/broom.html) 
# TODO: fix errors that are due to having too many variables to fit on too little data by training for just social factors (and gotten rid of multicolineary factors)
# TODO: possible also add a SGWR model to allow some variables to be stationary and hence reducing model complexity
gwr_model <- train_GWR(data=neighbourhood_data, model_recipe=model_rec, weight_func=gwr.Gauss)
```


```{r}
# Create the model plans, indicating which types of models to fit and what hyperparameters to tune

# Linear regression plan
lm_plan <- 
  linear_reg() %>% 
  set_engine("lm")

# Random forest plan
rf_plan <- parsnip::rand_forest() %>%
  parsnip::set_args(mtry  = tune()) %>%
  parsnip::set_args(min_n = tune()) %>%
  parsnip::set_args(trees = 2000) %>% # Setting the first search with 2000 trees # TODO: could tune this as well but that makes the tuning time much much longer probably
  parsnip::set_engine("ranger", importance = "permutation") %>% 
  parsnip::set_mode("regression")
```

For the hyperparameters that are set to be tuned above, set up the grid search combinations to use for tuning. 
```{r}
# Use the expansion option for setting the grids # TODO: decide on which option to select
# Random forest grid
rf_grid <- expand.grid (mtry = c(3,6,9), 
                       min_n = c(100,500,800)) # TODO: set this based on how many neighbourhoods we have
```

## Determine the best tuned hyperparameters and corresponding models
Now we can create the workflow for training the models to match the model recipe with the model plans. The created workflows are then used to tune the models to their grids to be able to extract the optimal hyperparameters and their corresponding models. This is done using the cross validation datasets. Extract the best hyperparameters from each above tuned models and their corresponding optimal models based on their training performance.

```{r}
#' Determine the best parameters for linear regression model using (spatial) cross validation with the training dataset, and make predictions on the validation set
#' 
#' @param lm_plan tidymodels plan of linear regression model which specifies the kind of model to train
#' @param model_recipe tidymodels recipe describing the equation to determine relations for
#' @param cv_folds (spatial) cross validation folds on the training dataset
#' 
#' @retun list(lm_best_params, lm_best_OOF_preds, lm_best_wf) where the first element contains the best coefficients for the linear regression model, the second element the predictions on the validation set based on the best model corresponding to these coefficients, and the third element the final workflow to be used for training on the full training dataset
lm_cross_validation <- function(lm_plan, model_recipe, cv_folds){
  # Set model workflows
  lm_wf <-
    workflows::workflow() %>% 
    add_recipe(model_recipe) %>% 
    add_model(lm_plan)
  
  # Tune linear regression coefficients (no hyperparameters to tune, but recipe coefficients instead; grid value states how many coefficient combinations need to be created automatically)
  lm_tuned <- tune_grid(lm_wf, resamples = cv_folds, grid = 10, control=control_grid(save_pred = TRUE))
  
  # Extract the best parameters based on RMSE, also used for evaluating performance on the test data
  lm_best_params <- select_best(lm_tuned, metric = "rmse")
  
  # Pull best hyperparameter combination from the 10-fold cross validated predictions
  lm_best_OOF_preds <- collect_predictions(lm_tuned)
  
  # Retrieve the best parameter model
  lm_best_OOF_preds <- lm_best_OOF_preds %>% dplyr::select(-id,-.config)
  
  # Create final workflow, used for the final training round on the full training dataset
  lm_best_wf <- finalize_workflow(lm_wf, lm_best_params)
  
  return (list(lm_best_params, lm_best_OOF_preds, lm_best_wf))
}
```

```{r}
# Use cross validation to determine the best linear regression model
lm_lst <- lm_cross_validation(lm_plan=lm_plan, model_recipe=model_rec, cv_folds=cv_splits)

lm_best_params <- lm_lst[[1]]
lm_best_OOF_preds <- lm_lst[[2]]
lm_best_wf <- lm_lst[[3]]
```

```{r}
# Use spatial cross validation to determine the best linear regression model
lm_lst_spatial <- lm_cross_validation(lm_plan=lm_plan, model_recipe=model_rec, cv_folds=cv_spatial_folds)

lm_best_params_spatial <- lm_lst_spatial[[1]]
lm_best_OOF_preds_spatial <- lm_lst_spatial[[2]]
lm_best_wf_spatial <- lm_lst_spatial[[3]]
```

```{r}
#' Determine the best hyperparameters for random forest model using (spatial) cross validation with the training dataset, and make predictions on the validation set
#' 
#' @param rf_plan tidymodels plan of random forest model which specifies the kind of model to train
#' @param rf_grid tidymodels grid with the hyperparameter value combinations to determine the best ones of using cross validation
#' @param model_recipe tidymodels recipe describing the equation to determine relations for
#' @param cv_folds (spatial) cross validation folds on the training dataset
#' 
#' @retun list(rf_best_params, rf_best_OOF_preds, rf_best_wf) where the first element contains the best hyperparameters for the random forest model, the second element the predictions on the validation set based on the best model corresponding to these parameters, and the third element the final workflow to be used for training on the full training dataset
rf_cross_validation <- function(rf_plan, rf_grid, model_recipe, cv_folds){
  # Set model workflows
  rf_wf <-
  workflows::workflow() %>% 
  add_recipe(model_rec) %>% 
  add_model(rf_plan)
  
  # Tune model hyperparameters
  rf_tuned <- tune_grid(rf_wf, resamples = cv_folds, grid = rf_grid, control=control_grid(save_pred = TRUE))
  
  # Extract the best parameters based on RMSE, also used for evaluating performance on the test data
  rf_best_params <- select_best(rf_tuned, metric = "rmse")
  
  # Pull best hyperparameter combinations from the 10-fold cross validated predictions
  rf_best_OOF_preds <- collect_predictions(rf_tuned) %>%
    filter(mtry  == rf_best_params$mtry[1] & min_n == rf_best_params$min_n[1])
  
  # Retrieve the best parameter models
  rf_best_OOF_preds <- rf_best_OOF_preds %>% dplyr::select(-id,-.config)
  
  # Create final workflow
  rf_best_wf <- finalize_workflow(rf_wf, rf_best_params)
  
  return (list(rf_best_params, rf_best_OOF_preds, rf_best_wf))
}
```

```{r}
# Use cross validation to determine the best random forest model
rf_lst <- rf_cross_validation(rf_plan=rf_plan, rf_grid=rf_grid, model_recipe=model_rec, cv_folds=cv_splits)

rf_best_params <- rf_lst[[1]]
rf_best_OOF_preds <- rf_lst[[2]]
rf_best_wf <- rf_lst[[3]]
```

```{r}
# Use spatial cross validation to determine the best random forest model
rf_lst_spatial <- rf_cross_validation(rf_plan=rf_plan, rf_grid=rf_grid, model_recipe=model_rec, cv_folds=cv_spatial_folds)

rf_best_params_spatial <- rf_lst_spatial[[1]]
rf_best_OOF_preds_spatial <- rf_lst_spatial[[2]]
rf_best_wf_spatial <- rf_lst_spatial[[3]]
```

Evaluate performance of these best tuned models on training data.
```{r}
# Make prediction for each model and attach error statistics
OOF_preds <- rbind(
  data.frame(lm_best_OOF_preds %>% dplyr::select(.pred,turnout_ratio),model = "Model-1a_Linear Regression"),
                   data.frame(lm_best_OOF_preds_spatial %>% dplyr::select(.pred,turnout_ratio),model = "Model-1b_Linear Regression spatial"),
                   data.frame(rf_best_OOF_preds %>% dplyr::select(.pred,turnout_ratio),model = "Model-3a_Random Forest"), 
                   data.frame(rf_best_OOF_preds_spatial %>% dplyr::select(.pred,turnout_ratio),model = "Model-3b_Random Forest spatial")) %>%
  group_by(model) %>% 
  mutate(
    RMSE = yardstick::rmse_vec(turnout_ratio, .pred),
    MAE  = yardstick::mae_vec(turnout_ratio, .pred),
    MAPE = yardstick::mape_vec((turnout_ratio+1), (.pred+1))) %>% 
  ungroup()
```

```{r}
# Plot average squared error (RMSE) for each model 
ggplot(data = OOF_preds %>%
         dplyr::select(model, RMSE) %>%
         distinct() ,
       aes(x = model, y = RMSE, group = 1)) +
  geom_path(color = "green") +
  geom_label(aes(label = round(RMSE,3))) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# Plot mean absolute error (MAE) for each model
ggplot(data = OOF_preds %>%
         dplyr::select(model, MAE) %>%
         distinct() ,
       aes(x = model, y = MAE, group = 1))  +
  geom_path(color = "red") +
  geom_label(aes(label = round(MAE,3))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# TODO: could also plot MAPE

# Plot predicted versus the observed values for each model in scatter plots
ggplot(OOF_preds, aes(y=.pred , x = turnout_ratio,group = model)) + 
  geom_point(alpha = 0.3) +
  coord_equal() +
  geom_abline(linetype = "dashed",color = "blue") +
  geom_smooth(method="lm", color = "red") +
  facet_wrap(~model,ncol = 3)+
  theme_bw()+
  xlab("Election turnout fraction") + 
  ylim(0, 10)+
  xlim(0, 10)
```

As can be seen, the different models predict differently and have varying level of training errors.

## Evaluate models on test data
To check how well the models generalise to unseen data, the predictions are made for the test data and corresponding errors are determined. For this the models with best parameters are first fit with the full training dataset and not only cross validation folds.

```{r}
#' Fit a final linear regression model based on the best coefficients and full training dataset and determine its predictions on the test dataset
#' 
#' @param lm_workflow the workflow of the linear regression model with the best coefficients and recipe
#' @param data_split a split into training and test dataset, respectively for training and prediction purposes 
#' 
#' @return list(lm_val_fit_geo, lm_val_pred_geo) list with as first element the final fit linear regression model, and as second element its predictions on the test dataset
lm_final_training <- function(lm_workflow, data_split){
  # Fit model on full training dataset
  lm_val_fit_geo <- lm_best_wf %>%
    last_fit(split     = data_split,
           metrics   = metric_set(yardstick::rmse, yardstick::rsq))
  
  # Using this final trained model, make predictions for the test dataset and get the best configuration for the test data for required model
  lm_val_pred_geo <- collect_predictions(lm_val_fit_geo)
  
  return (list(lm_val_fit_geo, lm_val_pred_geo))
}
```

```{r}
# Fit and test a final linear regression model for which its coefficients were determined without spatial cross validation
lm_fit_lst <- lm_final_training(lm_workflow=lm_best_wf, data_split=data_split)

lm_val_fit_geo <- lm_fit_lst[[1]]
lm_val_pred_geo <- lm_fit_lst[[2]]
```

```{r}
# Fit and test a final linear regression model for which its coefficients were determined with spatial cross validation
lm_fit_lst_spatial <- lm_final_training(lm_workflow=lm_best_wf_spatial, data_split=data_split)

lm_val_fit_geo_spatial <- lm_fit_lst_spatial[[1]]
lm_val_pred_geo_spatial <- lm_fit_lst_spatial[[2]]
```

```{r}
#' Fit a final random forest model based on the best hyperparameters and full training dataset and determine its predictions on the test dataset
#' 
#' @param rf_workflow the workflow of the random forest model with the best hyperparameters and recipe
#' @param data_split a split into training and test dataset, respectively for training and prediction purposes 
#' 
#' @return list(rf_val_fit_geo, rf_val_pred_geo) list with as first element the final fit random forest model, and as second element its predictions on the test dataset
rf_final_training <- function(rf_workflow, data_split){
  # Fit model on full training dataset
  rf_val_fit_geo <- rf_workflow %>% 
    last_fit(split     = data_split,
           metrics   = metric_set(yardstick::rmse, yardstick::rsq))
  
  # Using this final trained model, make predictions for the test dataset and get the best configuration for the test data for required model
  rf_val_pred_geo <- collect_predictions(rf_val_fit_geo)
  rf_val_pred_geo <- rf_val_pred_geo %>% dplyr::select(-id,-.config)
  
  return (list(rf_val_fit_geo, rf_val_pred_geo))
}
```

```{r}
# Fit and test a final random forest model for which its hyperparameters were determined without spatial cross validation
rf_fit_lst <- rf_final_training(rf_workflow=rf_best_wf, data_split=data_split)

rf_val_fit_geo <- rf_fit_lst[[1]]
rf_val_pred_geo <- rf_fit_lst[[2]]
```

```{r}
# Fit and test a final random forest model for which its hyperparameters were determined with spatial cross validation
rf_fit_lst_spatial <- rf_final_training(rf_workflow=rf_best_wf_spatial, data_split=data_split)

rf_val_fit_geo_spatial <- rf_fit_lst_spatial[[1]]
rf_val_pred_geo_spatial <- rf_fit_lst_spatial[[2]]
```


```{r}
# Aggregate test set predictions (these do not overlap with training prediction set, which is OOF_preds) with error statistics
val_preds <- rbind(
  data.frame(dplyr::select(lm_val_pred_geo, .pred, turnout_ratio), model = "Model-1a_Linear Regression"),
                   data.frame(dplyr::select(lm_val_pred_geo_spatial, .pred, turnout_ratio), model = "Model-1b_Linear Regression spatial"),
                   data.frame(dplyr::select(rf_val_pred_geo, .pred, turnout_ratio), model = "Model-3a_Random Forest"),
                   data.frame(dplyr::select(rf_val_pred_geo_spatial, .pred, turnout_ratio), model = "Model-3b_Random Forest spatial")) %>% 
  group_by(model) %>% 
  mutate(RMSE = yardstick::rmse_vec(turnout_ratio, .pred),
         MAE  = yardstick::mae_vec(turnout_ratio, .pred),
         MAPE = yardstick::mape_vec((turnout_ratio+1), (.pred+1)),
         absE=abs(turnout_ratio-.pred)) %>% 
  ungroup()
```

```{r}
# Plot average squared error (RMSE) for each model 
ggplot(data = val_preds %>% 
                           dplyr::select(model, RMSE) %>% 
                           distinct() , 
                         aes(x = model, y = RMSE, group = 1))  +
  geom_path(color = "green") +
  geom_label(aes(label = round(RMSE,4))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# Plot mean absolute error (MAE) for each model # TODO: may need to ylim the plots as can otherwise be misleading how much different the values are
ggplot(data = val_preds %>%
         dplyr::select(model, MAE) %>%
         distinct() ,
       aes(x = model, y = MAE,group = 1))  +
  geom_path(color = "red") +
  geom_label(aes(label = round(MAE,3))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# TODO: could also plot MAPE and absE, and possibly calculate the AIC heuristic, and may be nice to make a ROC curve
# TODO: may want to change xlim to make better visible

# Plot predicted versus the observed values for each model in scatter plots
ggplot(val_preds, aes(y=.pred , x = turnout_ratio,group = model))+ 
  geom_point(alpha = 0.3) +
  coord_equal() +
  geom_abline(linetype = "dashed",color = "red") +
  geom_smooth(method="lm", color = "blue") +
  facet_wrap(~model,ncol = 3)+
  theme_bw()+
  xlab("Election turnout fraction") + 
  ylim(0,10)+
  xlim(0,10)
```
As can be seen, like with the performance on training data, the different models predict differently and have varying level of testing errors.

## Check for spatial autocorrelation with linear regression model
Check if the assumptions of final trained linear regression model hold using this dataset (likely not), by investigating multicollinearity and spatial autocorrelation.
```{r}
# Check assumptions of linear regression like multicollinearity, which occurs when the predictor variables actually predict one another, and due to correlation some predictor variables may also be redundant
# Information from: https://jhudatascience.org/tidyversecourse/model.html 5.14.1.4 and https://cran.r-project.org/web/packages/performance/performance.pdf
lm_val_fit_geo %>% pluck(".workflow", 1) %>% extract_fit_parsnip() %>% check_model(panel=FALSE) # Without spatial cross validation
```

```{r} 
lm_val_fit_geo_spatial %>% pluck(".workflow", 1) %>% extract_fit_parsnip() %>% check_model(panel=FALSE) # With spatial cross validation
```

```{r}
# Check multicollinearity with vif of car package
lm_model <- lm_val_fit_geo %>% pluck(".workflow", 1) %>% extract_fit_engine()
lm_model %>% car::vif() # Without spatial cross validation
```

```{r}
# Check multicollinearity with vif of car package
lm_model_spatial <- lm_val_fit_geo_spatial %>% pluck(".workflow", 1) %>% extract_fit_engine() 
lm_model_spatial %>% car::vif() # With spatial cross validation
```

```{r}
# Check for spatial autocorrelation by first creating different kinds of weight matrixes describing neighbourhood relations

# Adjacency matrix using Euclidean distance between neighbourhood centroids 
# Because not all neighbourhoods are touching each other (not all have a turnout value), queens contiguity matrix does not work (with poly2nb function)
neighbours <- dnearneigh(centroid_coords, 0, 50000) # Defining as adjacent all observations between 0 and 50km
Wadj = nb2listw(neighbours, style = "W", zero.policy = TRUE) # Computing weight matrix W; style W row normalized; B non-normalized; zero.policy = TRUE means that some observations may have no adjacent neighbourhoods

# Distance-based weight matrix W: 1/d (note that we're only calculating distance-based weight for "adjacent" nodes)
dist = nbdists(neighbours,centroid_coords) # Determine distance between all neighbourhood centroids
ids = lapply(dist, function(x) 1/(x+0.00001)) # Note that we add 0.00001 in the denominator to avoid divisions by zero, which would have occurred when a node is not considered to adjacent (thus giving 0 instead of NA as weight)
Wids = nb2listw(neighbours, glist = ids, style = "W", zero.policy = TRUE)

# Distance-based weight matrix W: 1/d^2
ids2 = lapply(dist, function(x) 1/(x^2+0.00001))
Wids2 = nb2listw(neighbours, glist = ids2, style = "W", zero.policy = TRUE)

# Distance-based weight matrix W: exp(-x)
expdis <- lapply(dist, function(x) {
  tryCatch(exp(-x/50000), error = function(e) return(NULL))
}) # Note that the exponential function is scale dependent, and consequentially the number used to normalize the distances (in this case 50km; equal to the maximum distance for adjacent units) affects W

Wexpdis = nb2listw(neighbours, glist = expdis, style = "W", zero.policy = TRUE)
```

```{r}
# Plot adjacent neighbours with line between them
coordsW <- neighbourhood_data %>%
  st_centroid()%>%
  st_geometry()
plot(Wadj, st_geometry(coordsW), col="red")
```

```{r}
# Show summary of neighbourhood adjacency
summary(Wadj, zero.policy=TRUE) 

# Print weights of first few neighbourhoods
# print(weights(Wadj)[1:5],zero.policy=TRUE)
```

```{r}
#' Check spatial autocorrelation with linear regression model and create plots of this
#' 
#' @param fit_model final fit of linear regression model
#' @param weight_matrix weight matrix stating the neighbourhood relations to take into account
#' @param dataset full neighbourhood dataset
#' @param n_permutations (default 2999) number of permutations for Monte Carlo method to bootstrap different polygon distribution
#' @param suffix (default '') suffix used for the column name with residuals of each neighbourhood (otherwise it may be overwritten when different linear regression models were trained and inspected with this function)
#' 
#' @return lm_res map with the residuals plotted on the neighbourhoods
lm_check_autocorrelation <- function(fit_model, weight_matrix, dataset, test_data, training_data, test_predictions, n_permutations=2999, suffix=''){
  # TODO: set that 2999 to sensible number
  # Determine the residuals for the full dataset in the same order as the neighbourhoods in the weight matrix (not only for the training data as in fit_model$residuals (due to train-test split))
  # Compute the residuals of test data and match these with neighbourhood IDs
  test_residuals <- cbind(test_data$ID, test_predictions$turnout_ratio - test_predictions$.pred)
  # Match the residuals of training data with neighbourhood IDs
  training_residuals <- cbind(training_data$ID, fit_model$residuals)
  # Add the residuals dataframes together
  residuals <- rbind(test_residuals, training_residuals)
  # Add column names to residuals matrix
  colnames(residuals) <- c("ID", "residual")
  # Create dataframe out of residuals matrix
  residuals_df <- data.frame(residuals)
  # Sort the residuals based on the neighbourhood ID to get in the same order as the neighbourhoods in the weight matrix
  residuals_ordered <- residuals_df[order(residuals_df$ID),]
  
  # Because the moran.test function for retrieving the Moran's I is sensitive to irregularly distributed polygons, here we use a Monte Carlo method to bootstrap different polygon distribution (with moran.mc() function)
  mc_global <- moran.mc(residuals_ordered$residual, weight_matrix, n_permutations, alternative="greater")
  
  # Plot the  Moran's I
  plot(mc_global)
  
  print(mc_global) # When the p value is significant we reject the null hypothesis that there is no significant autocorrelations among the variable, and accept that, the residual has spatial clustering
  
  # Plot the residual on the neighbourhood polygons
  col_str <- paste("res_lm", suffix, sep="_") # Create column name using suffix
  dataset[col_str] <- residuals_ordered$residual
  lm_res <- mapview::mapview(dataset, 
                                zcol = col_str, 
                                col.regions=brewer.pal(8, "YlOrRd"),
                                at = c(-5,-1,-0.5,0,0.5,1,5))
    
    #qtm(dataset, col_str)
  
  return (lm_res)
}
```

```{r}
# Inspect spatial autocorrelation when linear regression coefficients were determined without spatial cross correlation, using adjacency matrix
lm_res_adj <- lm_check_autocorrelation(fit_model=lm_model, weight_matrix=Wadj, dataset=neighbourhood_data, test_data=test.set, training_data=train.set, test_predictions=lm_val_pred_geo, n_permutations=2999, suffix='')
```

```{r}
# Show residual plot using adjacency matrix and no spatial cross validation
lm_res_adj
```

```{r}
# Inspect spatial autocorrelation when linear regression coefficients were determined without spatial cross correlation, using inverse distance matrix
lm_res_ids <- lm_check_autocorrelation(fit_model=lm_model, weight_matrix=Wids, dataset=neighbourhood_data, test_data=test.set, training_data=train.set, test_predictions=lm_val_pred_geo, n_permutations=2999, suffix='')
```

```{r}
# Show residual plot using adjacency matrix and no spatial cross validation
lm_res_ids
```

```{r}
# Inspect spatial autocorrelation when linear regression coefficients were determined without spatial cross correlation, using inverse distance squared matrix
lm_res_ids2 <- lm_check_autocorrelation(fit_model=lm_model, weight_matrix=Wids2, dataset=neighbourhood_data, test_data=test.set, training_data=train.set, test_predictions=lm_val_pred_geo, n_permutations=2999, suffix='')
```

```{r}
# Show residual plot using adjacency matrix and no spatial cross validation
lm_res_ids2
```

```{r}
# Inspect spatial autocorrelation when linear regression coefficients were determined without spatial cross correlation, using inverse distance squared matrix
lm_res_expdis <- lm_check_autocorrelation(fit_model=lm_model, weight_matrix=Wexpdis, dataset=neighbourhood_data, test_data=test.set, training_data=train.set, test_predictions=lm_val_pred_geo, n_permutations=2999, suffix='')
```

```{r}
# Show residual plot using adjacency matrix and no spatial cross validation
lm_res_expdis
```

```{r}
# Inspect spatial autocorrelation when linear regression coefficients were determined with spatial cross correlation, using adjacency matrix
lm_res_adj_spatial <- lm_check_autocorrelation(fit_model=lm_model_spatial, weight_matrix=Wadj, dataset=neighbourhood_data, test_data=test.set, training_data=train.set, test_predictions=lm_val_pred_geo_spatial, n_permutations=2999, suffix='spatial')
```

```{r}
# Show residual plot using adjacency matrix and with spatial cross validation
lm_res_adj_spatial
```

For both linear regression models that are trained on a cross validation split with and without taking the spatial nature of the dataset into account, and for different weight matrices, significant spatial autocorrelation in the data can be concluded.

## Investigate non-stationarity with geographically weighted regression model
Now the geographically weighted regression models are trained, it is interesting to inspect the local patterns that have been found. For this purpose map the coefficients of interesting predictor variables and compare this to a map of the significance of this variable. 
```{r}
# Put the Geographically Weighted Regression with adaptive kernel results in data frame
gwr_out <- as.data.frame(gwr_val_fit_geo$SDF) # Without spatial cross-validation
gwr_out_spatial <- as.data.frame(gwr_val_fit_geo_spatial$SDF) # With spatial cross-validation
```

```{r}
# Join the local R2 for each GWR model to each neighborhood and display its distribution
# Without spatial cross-validation
neighbourhood_data$amb_localR2 <- gwr_out$localR2
mapview::mapview(neighbourhood_data, zcol = "amb_localR2", col.regions=brewer.pal(11, "RdYlGn")) 

# With spatial cross-validation
neighbourhood_data$amb_localR2_spatial <- gwr_out_spatial$localR2
mapview::mapview(neighbourhood_data, zcol = "amb_localR2_spatial", col.regions=brewer.pal(11, "RdYlGn")) 
```

```{r}
#' Create two maps in one venster with a slider to compare Geographically Weighted Regression coefficients of a predictor variable with its significance levels by first joining these values to their respective neighbourhood polygons
#' 
#' @param data Dataset with the polygon geometry information to which to join the coefficient and significance values
#' @param gwr_results Data frame with results of trained Geographically Weighted Regression model. This is different for the model trained using spatial cross-validation and when spatial was not taken into account for cross-validation
#' @param predictor String with the name of the predictor variable to create these maps for
#' @param suffix String (default "") with a suffix for the predictor name when the coefficient and significance values are added as column to data. This can uniquely refer to the model used for training to get unique column names, e.g. "spatial".
#' 
#' @return list(gwr_coef, gwr_sig) list with as first element a map with coefficient values, and as second element a map with significance values of this variable
map_coefficient_var_sig <- function(data, gwr_results, predictor, suffix="") {
  # Determine the coefficient variability over space for all relevant predictors, after joining that with the neighbourhood polygons. This plot will now only be created and later be shown together with its predictor significance plot
  # Create column name for adding coefficient values to data
  acoef_str <- paste("acoef", predictor, suffix, sep="_") 
  # Join coefficient values with neighbourhoods
  data[acoef_str] <- gwr_results[,predictor]
  # Create map of coefficient values
  gwr_coef <- mapview::mapview(data, zcol = acoef_str)
  
  # Compute and map the statistical significance for variable using t-test. Plot this next to the map with its respective coefficient value distribution
  # Create column names for adding significance values to data
  pred_str_se <- paste(predictor, "se", sep="_")
  pred_str_at <- paste("at", predictor, sep="_")
  pred_str_at_cat <- paste(pred_str_at, "cat", sep="_")
  # Estimate the t-value for variable 
  data$at_Green_avail = gwr_results[,predictor] / gwr_results[,pred_str_se]
  # Categorize the t-value to statistical significance
  data[pred_str_at_cat] <- cut(data[,pred_str_at],
                                    breaks=c(min(data[,pred_str_at]),
                                             -1.96, 1.96,
                                             max(data[,pred_str_at])),
                                    labels=c("sig","nonsig", "sig"))
  # Make plot of the variable significance 
  gwr_sig <- mapview::mapview(data, zcol = pred_str_at_cat)
  
  # Compare the coefficient and significance maps of this variable
  return(list(gwr_coef, gwr_sig))
}
```

```{r}
# TODO: get a variable that actually exists, and run this function for multiple others too
# Without spatial cross-validation
gwr_lst_education <- map_coefficient_var_sig(data=neighbourhood_data, gwr_results=gwr_out, predictor="education", suffix="")
gwr_coef_education <- gwr_list_education[[1]]
gwr_sig_education <- gwr_list_education[[2]]

# With spatial cross-validation
gwr_lst_spatial_education <- map_coefficient_var_sig(data=neighbourhood_data, gwr_results=gwr_out_spatial, predictor="education", suffix="spatial")
gwr_coef_spatial_education <- gwr_list_spatial_education[[1]]
gwr_sig_spatial_education <- gwr_list_spatial_education[[2]]
```

```{r}
# Compare maps of predictor coefficients and significance values # TODO: do this for others as well and may need to perhaps show this separately (without |)
gwr_coef_education | gwr_sig_education
```

## Inspect important variables of random forest model
To investigate what variables are most important for the Random Forest model to make its predictions, the feature importance is determined and plotted. This shows which variables were most important in predicting the turnout. The importance is first shown based on the impurity metric and after in an interactive model based on permutation, which is more robust. In this interactive model also the partial dependence of different variables can be explored to check their relations. Note that this can take quite a bit of time to run.
```{r}
# Plot the variable importance based on permutation (because on that is trained in tidymodels)
# Without spatial cross-validation
rf_val_fit_geo %>% pluck(".workflow", 1) %>% extract_fit_parsnip() %>%
vip::vip(num_features = 20, idth = 0.5, aesthetics = list(fill = "purple2"), include_type = T) # TODO: set num_features to sensible value for data 
# TODO: run also check_metrics()
```

```{r}
# Plot the variable importance based on permutation (because on that is trained in tidymodels)
# With spatial cross-validation
rf_val_fit_geo_spatial %>% pluck(".workflow", 1) %>% extract_fit_parsnip() %>% vip::vip(num_features = 19, idth = 0.5, aesthetics = list(fill = "purple2"), include_type = T) # TODO: set num_features to sensible value for data
```
